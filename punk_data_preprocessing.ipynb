{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from string import ascii_lowercase\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all csv files into one DataFrame (and csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(file_path):\n",
    "    '''\n",
    "    Takes in file path and returns a dataframe\n",
    "    Used for unknown delimiter\n",
    "    '''\n",
    "    # open csv file\n",
    "    csv_ = open(file_path)\n",
    "\n",
    "    # read in csv file\n",
    "    read = csv.reader(csv_)\n",
    "\n",
    "    # create empty list for dataframe\n",
    "    file = []\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    # read in each line\n",
    "    for lines in read:\n",
    "        # create/reset empty list for rows\n",
    "        row = []\n",
    "\n",
    "        # create/reset empty string for lyrics\n",
    "        lyr = ''\n",
    "\n",
    "        # create/reset accumulator\n",
    "        j = 0\n",
    "\n",
    "        for idx, i in enumerate(lines):\n",
    "\n",
    "            # lyrics start at index = 4\n",
    "            if idx > 3:\n",
    "                # strip punctuations except ' (and maybe ,)\n",
    "                i_no_punc = re.sub(r\"[^\\w\\s']\", '', i)\n",
    "\n",
    "                # add string to lyr\n",
    "                lyr += i_no_punc\n",
    "\n",
    "                # add to accumulator\n",
    "                j += 1\n",
    "\n",
    "                # subtract 4 for the 5 columns we want to end up with - 1 \n",
    "                # -1 since j accumulates before this statement\n",
    "                if j == len(lines) - 4:\n",
    "                    row.append(lyr)\n",
    "\n",
    "\n",
    "            # append every line that isn't lyrics    \n",
    "            else:\n",
    "                row.append(i)\n",
    "\n",
    "        \n",
    "        # append rows to file for dataframe\n",
    "        file.append(row)\n",
    "        \n",
    "    return pd.DataFrame(file[1:], columns = file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to directory with all files\n",
    "path = 'data/azlyrics-scraper/azlyrics_lyrics_'\n",
    "\n",
    "# create list of all paths, optional: add path for file for bands who start with a number\n",
    "all_paths = ['data/azlyrics-scraper/azlyrics_lyrics_19.csv']\n",
    "\n",
    "# loop through every letter in the alphabet to \n",
    "for char in ascii_lowercase:\n",
    "    # make new path to append\n",
    "    pathway = path + char + '.csv'\n",
    "    # append new path\n",
    "    all_paths.append(pathway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all_lyrics DF to concatenate other lyric dfs to\n",
    "all_lyrics = create_df('data/azlyrics-scraper/azlyrics_lyrics_19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every path, create a dataframe and concatenate it to all_lyrics\n",
    "# all_lyrics already has azlyrics_lyrics_19 in it, so 0th element is skipped\n",
    "for _ in all_paths[1:]:\n",
    "    df = create_df(_)\n",
    "    all_lyrics = pd.concat([all_lyrics, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index of all_lyrics\n",
    "all_lyrics.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save complete dataframe as csv\n",
    "all_lyrics.to_csv(path + 'complete.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating punk rock/pop band subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of pop punk bands - courtesy of Wikipedia (with some additions + deletions)\n",
    "bands_punk = [\"+44\", \"22 jacks\", \"5 seconds of summer\", \"999\", \"a\",\\\n",
    "              \"a day to remember\", \"the academy is...\", \"ace troubleshooter\",\\\n",
    "              \"acceptance\", \"afi\", \"against the current\", \"a change of pace\",\\\n",
    "              \"alien ant farm\", \"alkaline trio\", \"a loss for words\", \"all\",\\\n",
    "              \"the all-american rejects\", \"all time low\", \"allister\",\\\n",
    "              \"amber pacific\", \"american hi-fi\", \"army of freshmen\", \"anarbor\", \\\n",
    "              \"artist vs. poet\", \"as it is\", \"ash\", \"the ataris\", \"the audition\",\\\n",
    "              \"audio karate\", \"autopilot off\", \"avril lavigne\", \"beat crusaders\",\\\n",
    "              \"beat union\", \"better luck next time\", \"big drill car\", \"billy talent\",\\\n",
    "              \"blink-182\", \"bodyjar\", \"boris the sprinkler\", \"bomb the music industry!\",\\\n",
    "              \"the bouncing souls\", \"bowling for soup\", \"box car racer\", \"the boys\",\\\n",
    "              \"boys like girls\", \"bracket\", \"brand new\", \"broadside\", \"broadway calls\",\\\n",
    "              \"busted\", \"buzzcocks\", \"the cab\", \"carousel kings\", \"cartel\", \"cayetana\",\\\n",
    "              \"chasing morgan\", \"chixdiggit\", \"chunk! no, captain chunk!\", \"the click five\",\\\n",
    "              \"the copyrights\", \"count the stars\", \"courage my love\", \"crash romeo\", \"crimpshrine\",\\\n",
    "              \"cub\", \"cute is what we aim for\", \"daggermouth\", \"the dangerous summer\",\\\n",
    "              \"david crowder band\", \"the dead milkmen\", \"descendents\", \"the donnas\", \"the downtown fiction\",\\\n",
    "              \"driving east\", \"dum dums\", \"eat me raw\", \"every avenue\", \"eleventyseven\", \"elliot minor\",\\\n",
    "              \"energy\", \"the ergs!\", \"eternal boy\", \"eve 6\", \"even in blackouts\", \"everyday sunday\",\\\n",
    "              \"faber drive\", \"face to face\", \"falling in reverse\", \"fall out boy\", \"farewell\", \"fenix tx\",\\\n",
    "              \"fight fair\", \"finley\", \"fireworks\", \"flatfoot 56\", \"flop\", \"fluf\", \"fm static\", \"fonzie\",\\\n",
    "              \"forever came calling\", \"forever the sickest kids\", \"four year strong\", \"the friday night boys\",\\\n",
    "              \"generation x\", \"the get up kids\", \"ghoti hook\", \"go betty go\", \"go radio\", \"gob\", \"goldfinger\",\\\n",
    "              \"good charlotte\", \"goodnight nurse\", \"grayscale\", \"green day\", \"greyfield\", \"groovie ghoulies\",\\\n",
    "              \"guttermouth\", \"hagfish\", \"halifax\", \"handguns\", \"hangnail\", \"hawk nelson\", \"head injuries\",\\\n",
    "              \"hedley\", \"hey monday\", \"hey violet\", \"hidden in plain view\", \"the hi-fives\", \"the high court\",\\\n",
    "              \"the higher\", \"the hippos\", \"hit the lights\", \"home grown\", \"hot mulligan\", \"house of heroes\",\\\n",
    "              \"houston calls\", \"the huntingtons\", \"i call fives\", \"i fight dragons\", \"icon for hire\",\\\n",
    "              \"ivoryline\", \"the jam\", \"jawbreaker\", \"jeff rosenstock\", \"jimmy eat world\", \"june\",\\\n",
    "              \"just surrender\", \"karate high school\", \"kid down\", \"kids can't fly\", \"kids in glass houses\",\\\n",
    "              \"kids in the way\", \"killerpilze\", \"kiros\", \"kisschasy\", \"knuckle puck\", \"koopa\", \"lagwagon\",\\\n",
    "              \"latterman\", \"left front tire\", \"the leftovers\", \"the lemonheads\", \"less than jake\", \"lifesavors\",\\\n",
    "              \"lifetime\", \"light years\", \"like pacific\", \"the lillingtons\", \"limbeck\", \"lit\", \"lola ray\",\\\n",
    "              \"love you to death\", \"lucky 7\", \"lucky boys confusion\", \"ludo\", \"lustra\", \"mach pelican\",\\\n",
    "              \"magnapop\", \"the maine\", \"makeout\", \"man overboard\", \"manges\", \"marianas trench\",\\\n",
    "              \"masked intruder\", \"the matches\", \"mayday parade\", \"mcbusted\", \"mcfly\",\\\n",
    "              \"me first and the gimme gimmes\", \"melody fall\", \"mest\", \"the methadones\", \"midtown\",\\\n",
    "              \"millencolin\", \"mixtapes\", \"moose blood\", \"momoiro clover z\", \"motion city soundtrack\",\\\n",
    "              \"modern baseball\", \"the movielife\", \"the mr. t experience\", \"the muffs\", \"mxpx\",\\\n",
    "              \"my chemical romance\", \"neck deep\", \"nerf herder\", \"the new cities\", \"new found glory\",\\\n",
    "              \"new years day\", \"nofx\", \"noise by numbers\", \"not by choice\", \"no use for a name\", \"october fall\",\\\n",
    "              \"the offspring\", \"on my honor\", \"one ok rock\", \"orange\", \"over it\", \"paige\", \"parasites\",\\\n",
    "              \"paramore\", \"pardon us\", \"panic! at the disco\", \"patent pending\", \"permanent me\",\\\n",
    "              \"pee wee gaskins\", \"philmont\", \"pierce the veil\", \"plain white t's\", \"plinko\", \"pointed sticks\",\\\n",
    "              \"poor old lu\", \"the presidents of the united states of america\", \"propagandhi\", \"punchbuggy\",\\\n",
    "              \"punchline\", \"pup\", \"the queers\", \"quietdrive\", \"ramones\", \"rancid\", \"real friends\",\\\n",
    "              \"red city radio\", \"the red jumpsuit apparatus\", \"reggie and the full effect\", \"relient k\",\\\n",
    "              \"riddlin' kids\", \"roam\", \"rookie of the year\", \"rudi\", \"rufio\", \"run kid run\", \"saves the day\",\\\n",
    "              \"say anything\", \"saving aimee\", \"scenes from a movie\", \"scott murphy\", \"screeching weasel\",\\\n",
    "              \"seaway\", \"senses fail\", \"set it off\", \"set your goals\", \"shook ones\", \"short stack\", \"showoff\",\\\n",
    "              \"simple plan\", \"sleeping with sirens\", \"sleep on it\", \"slick shoes\", \"sludgeworth\", \"smash mouth\",\\\n",
    "              \"smoking popes\", \"something corporate\", \"son of dork\", \"the soviettes\", \"spanish love songs\",\\\n",
    "              \"sparks the rescue\", \"spazzys\", \"split habit\", \"sr-71\", \"stand atlantic\", \"the starting line\",\\\n",
    "              \"state champs\", \"stellar kart\", \"stereos\", \"steriogram\", \"stiff dylans\", \"story of the year\",\\\n",
    "              \"the story so far\", \"story untold\", \"student rick\", \"sugarcult\", \"sum 41\", \"supergrass\",\\\n",
    "              \"superman is dead\", \"the summer obsession\", \"the summer set\", \"sweet baby\", \"the swellers\",\\\n",
    "              \"tacocat\", \"taking back sunday\", \"teen idols\", \"teenage bottlerocket\", \"ten second epic\",\\\n",
    "              \"terrible things\", \"there for tomorrow\", \"these kids wear crowns\", \"this century\",\\\n",
    "              \"this time next year\", \"tigers jaw\", \"tilt\", \"title fight\", \"tonight alive\", \"transit\",\\\n",
    "              \"trash boat\", \"the travoltas\", \"treble charger\", \"trucks\", \"twenty twenty\", \"undercover\",\\\n",
    "              \"the undertones\", \"the unlovables\", \"unwritten law\", \"valencia\", \"the vandals\", \"vanilla sky\",\\\n",
    "              \"verona grove\", \"violent delight\", \"waterparks\", \"wavves\", \"wakefield\", \"we are the in crowd\",\\\n",
    "              \"the wedding\", \"weezer\", \"we the kings\", \"wheatus\", \"with confidence\", \"the wonder years\",\\\n",
    "              \"wstr\", \"yellowcard\", \"you, me, and everyone we know\", \"you me at six\", \"zebrahead\",\\\n",
    "              \"zolof the rock and roll destroyer\", \"menzingers, the\", \"anberlin\", 'silverstein', 'rise against',\\\n",
    "              \"the used\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure name of bands with \"the\" in front to match dataframe style\n",
    "rest_bands = []\n",
    "for i in bands_punk:\n",
    "    # len of 'the ' is 4; check if first 4 characters are 'the '\n",
    "    if i[:4] == 'the ':\n",
    "        # take out 'the ' from band name and place at end of string\n",
    "        restructured = i[4:] + \", the\"\n",
    "        # append to new list\n",
    "        rest_bands.append(restructured)\n",
    "    elif i[:2] == 'a ':\n",
    "        # take out 'a ' from band name and place at end of string\n",
    "        restructured = i[2:] + \", a\"\n",
    "        # append to new list\n",
    "        rest_bands.append(restructured)\n",
    "    else:\n",
    "        # append name of bands without 'the ' in front\n",
    "        rest_bands.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create punk_df which has the punk rock/pop bands from our list of bands\n",
    "punk_df = all_lyrics[all_lyrics.ARTIST_NAME.isin(rest_bands)]\n",
    "punk_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv\n",
    "punk_df.to_csv('data/punk_bands.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARTIST_NAME</th>\n",
       "      <th>ARTIST_URL</th>\n",
       "      <th>SONG_NAME</th>\n",
       "      <th>SONG_URL</th>\n",
       "      <th>LYRICS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 seconds of summer</td>\n",
       "      <td>https://www.azlyrics.com/19/5secondsofsummer.html</td>\n",
       "      <td>gotta get out</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/5secondsofsumm...</td>\n",
       "      <td>even when the sky is falling down even when th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5 seconds of summer</td>\n",
       "      <td>https://www.azlyrics.com/19/5secondsofsummer.html</td>\n",
       "      <td>better man</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/5secondsofsumm...</td>\n",
       "      <td>find me at a quarter to three cigarette in my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 seconds of summer</td>\n",
       "      <td>https://www.azlyrics.com/19/5secondsofsummer.html</td>\n",
       "      <td>more</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/5secondsofsumm...</td>\n",
       "      <td>if me and you are living in the same place why...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 seconds of summer</td>\n",
       "      <td>https://www.azlyrics.com/19/5secondsofsummer.html</td>\n",
       "      <td>why won't you love me</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/5secondsofsumm...</td>\n",
       "      <td>switching into airplane mode again we're not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 seconds of summer</td>\n",
       "      <td>https://www.azlyrics.com/19/5secondsofsummer.html</td>\n",
       "      <td>woke up in japan</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/5secondsofsumm...</td>\n",
       "      <td>i woke up in japan feeling low feeling lonely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>zebrahead</td>\n",
       "      <td>https://www.azlyrics.com/z/zebrahead.html</td>\n",
       "      <td>out of control</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/zebrahead/outo...</td>\n",
       "      <td>i'm a mad man with a mission like a nightmare ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>zebrahead</td>\n",
       "      <td>https://www.azlyrics.com/z/zebrahead.html</td>\n",
       "      <td>photographs</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/zebrahead/phot...</td>\n",
       "      <td>i am alive i am awake not like i'm not aware i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>zebrahead</td>\n",
       "      <td>https://www.azlyrics.com/z/zebrahead.html</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/zebrahead/poli...</td>\n",
       "      <td>firewall school halls cop cars protecting you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>zebrahead</td>\n",
       "      <td>https://www.azlyrics.com/z/zebrahead.html</td>\n",
       "      <td>with legs like that</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/zebrahead/with...</td>\n",
       "      <td>here she comes again like good medicine every ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>zebrahead</td>\n",
       "      <td>https://www.azlyrics.com/z/zebrahead.html</td>\n",
       "      <td>wookie</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/zebrahead/wook...</td>\n",
       "      <td>in a cantina on tatoinea a kereoki wookie sing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ARTIST_NAME                                         ARTIST_URL  \\\n",
       "0     5 seconds of summer  https://www.azlyrics.com/19/5secondsofsummer.html   \n",
       "1     5 seconds of summer  https://www.azlyrics.com/19/5secondsofsummer.html   \n",
       "2     5 seconds of summer  https://www.azlyrics.com/19/5secondsofsummer.html   \n",
       "3     5 seconds of summer  https://www.azlyrics.com/19/5secondsofsummer.html   \n",
       "4     5 seconds of summer  https://www.azlyrics.com/19/5secondsofsummer.html   \n",
       "...                   ...                                                ...   \n",
       "2512            zebrahead          https://www.azlyrics.com/z/zebrahead.html   \n",
       "2513            zebrahead          https://www.azlyrics.com/z/zebrahead.html   \n",
       "2514            zebrahead          https://www.azlyrics.com/z/zebrahead.html   \n",
       "2515            zebrahead          https://www.azlyrics.com/z/zebrahead.html   \n",
       "2516            zebrahead          https://www.azlyrics.com/z/zebrahead.html   \n",
       "\n",
       "                  SONG_NAME  \\\n",
       "0             gotta get out   \n",
       "1                better man   \n",
       "2                      more   \n",
       "3     why won't you love me   \n",
       "4          woke up in japan   \n",
       "...                     ...   \n",
       "2512         out of control   \n",
       "2513            photographs   \n",
       "2514               politics   \n",
       "2515    with legs like that   \n",
       "2516                 wookie   \n",
       "\n",
       "                                               SONG_URL  \\\n",
       "0     https://www.azlyrics.com/lyrics/5secondsofsumm...   \n",
       "1     https://www.azlyrics.com/lyrics/5secondsofsumm...   \n",
       "2     https://www.azlyrics.com/lyrics/5secondsofsumm...   \n",
       "3     https://www.azlyrics.com/lyrics/5secondsofsumm...   \n",
       "4     https://www.azlyrics.com/lyrics/5secondsofsumm...   \n",
       "...                                                 ...   \n",
       "2512  https://www.azlyrics.com/lyrics/zebrahead/outo...   \n",
       "2513  https://www.azlyrics.com/lyrics/zebrahead/phot...   \n",
       "2514  https://www.azlyrics.com/lyrics/zebrahead/poli...   \n",
       "2515  https://www.azlyrics.com/lyrics/zebrahead/with...   \n",
       "2516  https://www.azlyrics.com/lyrics/zebrahead/wook...   \n",
       "\n",
       "                                                 LYRICS  \n",
       "0     even when the sky is falling down even when th...  \n",
       "1     find me at a quarter to three cigarette in my ...  \n",
       "2     if me and you are living in the same place why...  \n",
       "3      switching into airplane mode again we're not ...  \n",
       "4     i woke up in japan feeling low feeling lonely ...  \n",
       "...                                                 ...  \n",
       "2512  i'm a mad man with a mission like a nightmare ...  \n",
       "2513  i am alive i am awake not like i'm not aware i...  \n",
       "2514  firewall school halls cop cars protecting you ...  \n",
       "2515  here she comes again like good medicine every ...  \n",
       "2516  in a cantina on tatoinea a kereoki wookie sing...  \n",
       "\n",
       "[2517 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create seed for reusability\n",
    "random.seed(a = 11)\n",
    "\n",
    "#create list of subset of punk bands\n",
    "subset_punk = []\n",
    "\n",
    "# subset only using 1/3 bands found in the dataframe\n",
    "while len(subset_punk) < round(len(np.unique(punk_df.ARTIST_NAME))/3):\n",
    "    # randomly choose a band from the list\n",
    "    chosen = random.choice(np.unique(punk_df.ARTIST_NAME))\n",
    "    # if band not in the list, append the band to the list\n",
    "    if chosen not in subset_punk:\n",
    "        subset_punk.append(chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_punk_df = all_lyrics[all_lyrics.ARTIST_NAME.isin(subset_punk)]\n",
    "subset_punk_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv\n",
    "subset_punk_df.to_csv('data/subset_punk_bands.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pseudo Code for splitting lyrics (based on commas)\n",
    "```python\n",
    "split_lyrics_lst= []\n",
    "for row in punk df:\n",
    "    split_lyr = row.split(', ')\n",
    "    rows = []\n",
    "    for line in split_lyr:\n",
    "        rows.append(name, song name, line)\n",
    "        \n",
    "    split_lyrics_lst.append(rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Input, Embedding, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"randomly\" choose one band for model\n",
    "yellowcard = punk_df.LYRICS[punk_df.ARTIST_NAME == 'yellowcard']\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(yellowcard)\n",
    "\n",
    "token_seq = tokenizer.texts_to_sequences(yellowcard)\n",
    "\n",
    "n_gram_seq = []\n",
    "# for every line in tokenized sequences\n",
    "for line in token_seq:\n",
    "    # used to append the token_seq starting from 0th element to 1st element\n",
    "    for length in range(2, len(line)):\n",
    "        n_gram_seq.append(line[:length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create padded sequences\n",
    "n_gram_seq_padded = pad_sequences(n_gram_seq, maxlen = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0, 704,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 704,   3, 108],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 704,   3, 108, 158],\n",
       "       [  0,   0,   0,   0,   0,   0, 704,   3, 108, 158,  14],\n",
       "       [  0,   0,   0,   0,   0, 704,   3, 108, 158,  14,   9],\n",
       "       [  0,   0,   0,   0, 704,   3, 108, 158,  14,   9, 204],\n",
       "       [  0,   0,   0, 704,   3, 108, 158,  14,   9, 204,   1],\n",
       "       [  0,   0, 704,   3, 108, 158,  14,   9, 204,   1,  63],\n",
       "       [  0, 704,   3, 108, 158,  14,   9, 204,   1,  63, 113],\n",
       "       [704,   3, 108, 158,  14,   9, 204,   1,  63, 113,   8]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_seq_padded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels by using One Hot Encoding \n",
    "labels = to_categorical(n_gram_seq_padded[:,-1:])\n",
    "X = n_gram_seq_padded[:,:-1]\n",
    "\n",
    "train_size = round(n_gram_seq_padded.shape[0]*0.8)\n",
    "\n",
    "# create test and train\n",
    "y_train = labels[:train_size:]\n",
    "y_test = labels[train_size:,:]\n",
    "\n",
    "x_train = X[:train_size,:]\n",
    "x_test = X[train_size:,:]\n",
    "\n",
    "# find largest vocab size in padded sequence; this is input size\n",
    "vocab_size = max([w for sentence in n_gram_seq_padded for w in sentence]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = 64, input_length = 11))\n",
    "\n",
    "# add bidirectional lstm layer\n",
    "model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
    "\n",
    "# add dropout layer\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# add another LSTM layer\n",
    "model.add(LSTM(64))\n",
    "\n",
    "model.add(Dense(round(vocab_size/2), activation = 'relu'))\n",
    "\n",
    "# potentially add embedding layer\n",
    "\n",
    "model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')\n",
    "\n",
    "# Stops training if validation loss doesn't improve\n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 11, 64)            74304     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 11, 256)           197632    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 11, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                82176     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 580)               37700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1161)              674541    \n",
      "=================================================================\n",
      "Total params: 1,066,353\n",
      "Trainable params: 1,066,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/belindanguyen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 40s 143ms/step - loss: 6.1875 - accuracy: 0.0468 - val_loss: 5.8822 - val_accuracy: 0.0498\n",
      "Epoch 2/20\n",
      "281/281 [==============================] - 40s 143ms/step - loss: 5.4612 - accuracy: 0.0540 - val_loss: 5.8352 - val_accuracy: 0.0587\n",
      "Epoch 3/20\n",
      "281/281 [==============================] - 39s 138ms/step - loss: 5.2003 - accuracy: 0.0564 - val_loss: 5.8996 - val_accuracy: 0.0405\n",
      "Epoch 4/20\n",
      "281/281 [==============================] - 42s 149ms/step - loss: 4.9428 - accuracy: 0.0774 - val_loss: 6.0174 - val_accuracy: 0.0628\n",
      "Epoch 5/20\n",
      "281/281 [==============================] - 38s 134ms/step - loss: 4.6774 - accuracy: 0.0946 - val_loss: 6.1232 - val_accuracy: 0.0619\n",
      "Epoch 6/20\n",
      "281/281 [==============================] - 38s 137ms/step - loss: 4.4150 - accuracy: 0.1183 - val_loss: 6.3707 - val_accuracy: 0.0694\n",
      "Epoch 7/20\n",
      "281/281 [==============================] - 41s 146ms/step - loss: 4.0891 - accuracy: 0.1653 - val_loss: 6.5457 - val_accuracy: 0.0645\n",
      "Epoch 8/20\n",
      "281/281 [==============================] - 40s 143ms/step - loss: 3.7748 - accuracy: 0.2031 - val_loss: 6.7745 - val_accuracy: 0.0601\n",
      "Epoch 9/20\n",
      "281/281 [==============================] - 41s 145ms/step - loss: 3.4427 - accuracy: 0.2470 - val_loss: 7.3667 - val_accuracy: 0.0672\n",
      "Epoch 10/20\n",
      "281/281 [==============================] - 43s 152ms/step - loss: 3.1661 - accuracy: 0.2928 - val_loss: 7.6631 - val_accuracy: 0.0681\n",
      "Epoch 11/20\n",
      "281/281 [==============================] - 50s 177ms/step - loss: 2.8722 - accuracy: 0.3465 - val_loss: 7.9035 - val_accuracy: 0.0721\n",
      "Epoch 12/20\n",
      "281/281 [==============================] - 43s 153ms/step - loss: 2.5868 - accuracy: 0.4094 - val_loss: 8.8642 - val_accuracy: 0.0645\n",
      "Epoch 13/20\n",
      "281/281 [==============================] - 50s 177ms/step - loss: 2.2947 - accuracy: 0.4561 - val_loss: 9.5173 - val_accuracy: 0.0681\n",
      "Epoch 14/20\n",
      "281/281 [==============================] - 37s 130ms/step - loss: 2.1182 - accuracy: 0.4893 - val_loss: 9.7197 - val_accuracy: 0.0641\n",
      "Epoch 15/20\n",
      "281/281 [==============================] - 53s 186ms/step - loss: 1.9183 - accuracy: 0.5293 - val_loss: 10.3129 - val_accuracy: 0.0574\n",
      "Epoch 16/20\n",
      "281/281 [==============================] - 44s 155ms/step - loss: 1.6883 - accuracy: 0.5791 - val_loss: 10.8092 - val_accuracy: 0.0610\n",
      "Epoch 17/20\n",
      "281/281 [==============================] - 49s 174ms/step - loss: 1.5340 - accuracy: 0.6099 - val_loss: 11.4419 - val_accuracy: 0.0565\n",
      "Epoch 18/20\n",
      "281/281 [==============================] - 49s 175ms/step - loss: 1.3274 - accuracy: 0.6638 - val_loss: 12.1983 - val_accuracy: 0.0583\n",
      "Epoch 19/20\n",
      "281/281 [==============================] - 49s 175ms/step - loss: 1.2021 - accuracy: 0.6884 - val_loss: 12.2804 - val_accuracy: 0.0538\n",
      "Epoch 20/20\n",
      "281/281 [==============================] - 64s 227ms/step - loss: 1.0964 - accuracy: 0.7118 - val_loss: 12.8097 - val_accuracy: 0.0610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fed9e67acd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 20, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyrics(prompt, length):\n",
    "    '''\n",
    "    prompt: string of lyrics\n",
    "    length: length of lyrics that is wanted (includes prompt)\n",
    "    '''\n",
    "    # edge case; if prompt is as long as the length wanted\n",
    "    if len(prompt.split(' ')) == length:\n",
    "        return prompt\n",
    "    else:\n",
    "        for _ in range(length - len(prompt.split(' '))):\n",
    "            \n",
    "            token_list = tokenizer.texts_to_sequences([prompt])[0]\n",
    "            token_padded = pad_sequences([token_list], maxlen = 11)\n",
    "            \n",
    "            # get predicted probability for each word\n",
    "            predicted_probs = model.predict(token_padded)[0]\n",
    "            \n",
    "            # find max probability of the each word\n",
    "            word_choice = predicted_probs.argmax()\n",
    "            \n",
    "            # find out what the word is\n",
    "            output_word = tokenizer.index_word[word_choice]\n",
    "            \n",
    "            # add word to the prompt\n",
    "            prompt += ' ' + output_word\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"eyes are feeling heavy but they never seem to close the same then i've always been here on the while\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lyrics(\"eyes are feeling heavy but they never seem to close\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"there's a place off ocean avenue you even here you've always climb up and i learned to want to say\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lyrics(\"there's a place off ocean avenue\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate tags with a number\n",
    "tags_index = dict()\n",
    "for idx, val in enumerate(np.unique(punk_df.ARTIST_NAME)):\n",
    "    tags_index[val] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_docs = []\n",
    "\n",
    "for idx, val in tags_index.items():\n",
    "    # create list for all lyrics\n",
    "    all_lyrics = []\n",
    "    # for all the lyrics of the songs the artist made\n",
    "    for i in punk_df.LYRICS[punk_df.ARTIST_NAME == idx].index:\n",
    "        # tokenize and append each song's lyrics to list\n",
    "        all_lyrics.append(nltk.word_tokenize(punk_df.LYRICS[punk_df.ARTIST_NAME == idx][i]))\n",
    "    # tag all tokenized lyrics with artist's name and append to tagged_docs\n",
    "    tagged_docs.append(TaggedDocument(all_lyrics, tags = idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(epochs = 20, hs = 1, alpha = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b394f7d0f05e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_dbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    926\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[1;32m    927\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m             \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m         )\n\u001b[1;32m    930\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, documents, corpus_file, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0mdocument_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m                 \u001b[0m_note_doctag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "model_dbow.build_vocab(documents = [x for x in tagged_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type([docs for docs in tagged_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"I love machine learning. Its awesome.\",\n",
    "        \"I love coding in python\",\n",
    "        \"I love building chatbots\",\n",
    "        \"they chat amagingly well\"]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=nltk.word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b9eac0affbb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    926\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[1;32m    927\u001b[0m             \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m             \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m         )\n\u001b[1;32m    930\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, documents, corpus_file, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m                     logger.warning(\n\u001b[1;32m   1056\u001b[0m                         \u001b[0;34m\"Each 'words' should be a list of words (usually unicode strings). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "m = Doc2Vec()\n",
    "  \n",
    "m.build_vocab(tagged_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'love', 'machine', 'learning', '.', 'its', 'awesome', '.'], tags=['0']),\n",
       " TaggedDocument(words=['i', 'love', 'coding', 'in', 'python'], tags=['1']),\n",
       " TaggedDocument(words=['i', 'love', 'building', 'chatbots'], tags=['2']),\n",
       " TaggedDocument(words=['they', 'chat', 'amagingly', 'well'], tags=['3'])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
