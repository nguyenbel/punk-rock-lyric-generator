{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from string import ascii_lowercase\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all csv files into one DataFrame (and csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(file_path):\n",
    "    '''\n",
    "    Takes in file path and returns a dataframe\n",
    "    Used for unknown delimiter\n",
    "    '''\n",
    "    # open csv file\n",
    "    csv_ = open(file_path)\n",
    "\n",
    "    # read in csv file\n",
    "    read = csv.reader(csv_)\n",
    "\n",
    "    # create empty list for dataframe\n",
    "    file = []\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    # read in each line\n",
    "    for lines in read:\n",
    "        # create/reset empty list for rows\n",
    "        row = []\n",
    "\n",
    "        # create/reset empty string for lyrics\n",
    "        lyr = ''\n",
    "\n",
    "        # create/reset accumulator\n",
    "        j = 0\n",
    "\n",
    "        for idx, i in enumerate(lines):\n",
    "\n",
    "            # lyrics start at index = 4\n",
    "            if idx > 3:\n",
    "                # strip punctuations except ' (and maybe ,)\n",
    "                i_no_punc = re.sub(r\"[^\\w\\s']\", '', i)\n",
    "\n",
    "                # add string to lyr\n",
    "                lyr += i_no_punc\n",
    "\n",
    "                # add to accumulator\n",
    "                j += 1\n",
    "\n",
    "                # subtract 4 for the 5 columns we want to end up with - 1 \n",
    "                # -1 since j accumulates before this statement\n",
    "                if j == len(lines) - 4:\n",
    "                    row.append(lyr)\n",
    "\n",
    "\n",
    "            # append every line that isn't lyrics    \n",
    "            else:\n",
    "                row.append(i)\n",
    "\n",
    "        \n",
    "        # append rows to file for dataframe\n",
    "        file.append(row)\n",
    "        \n",
    "    return pd.DataFrame(file[1:], columns = file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to directory with all files\n",
    "path = 'data/azlyrics-scraper/azlyrics_lyrics_'\n",
    "\n",
    "# create list of all paths, optional: add path for file for bands who start with a number\n",
    "all_paths = ['data/azlyrics-scraper/azlyrics_lyrics_19.csv']\n",
    "\n",
    "# loop through every letter in the alphabet to \n",
    "for char in ascii_lowercase:\n",
    "    # make new path to append\n",
    "    pathway = path + char + '.csv'\n",
    "    # append new path\n",
    "    all_paths.append(pathway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all_lyrics DF to concatenate other lyric dfs to\n",
    "all_lyrics = create_df('data/azlyrics-scraper/azlyrics_lyrics_19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every path, create a dataframe and concatenate it to all_lyrics\n",
    "# all_lyrics already has azlyrics_lyrics_19 in it, so 0th element is skipped\n",
    "for _ in all_paths[1:]:\n",
    "    df = create_df(_)\n",
    "    all_lyrics = pd.concat([all_lyrics, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index of all_lyrics\n",
    "all_lyrics.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save complete dataframe as csv\n",
    "all_lyrics.to_csv(path + 'complete.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating punk rock/pop band subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of pop punk bands - courtesy of Wikipedia (with some additions + deletions)\n",
    "bands_punk = [\"+44\", \"22 jacks\", \"5 seconds of summer\", \"999\", \"a\",\\\n",
    "              \"a day to remember\", \"the academy is...\", \"ace troubleshooter\",\\\n",
    "              \"acceptance\", \"afi\", \"against the current\", \"a change of pace\",\\\n",
    "              \"alien ant farm\", \"alkaline trio\", \"a loss for words\", \"all\",\\\n",
    "              \"the all-american rejects\", \"all time low\", \"allister\",\\\n",
    "              \"amber pacific\", \"american hi-fi\", \"army of freshmen\", \"anarbor\", \\\n",
    "              \"artist vs. poet\", \"as it is\", \"ash\", \"the ataris\", \"the audition\",\\\n",
    "              \"audio karate\", \"autopilot off\", \"avril lavigne\", \"beat crusaders\",\\\n",
    "              \"beat union\", \"better luck next time\", \"big drill car\", \"billy talent\",\\\n",
    "              \"blink-182\", \"bodyjar\", \"boris the sprinkler\", \"bomb the music industry!\",\\\n",
    "              \"the bouncing souls\", \"bowling for soup\", \"box car racer\", \"the boys\",\\\n",
    "              \"boys like girls\", \"bracket\", \"brand new\", \"broadside\", \"broadway calls\",\\\n",
    "              \"busted\", \"buzzcocks\", \"the cab\", \"carousel kings\", \"cartel\", \"cayetana\",\\\n",
    "              \"chasing morgan\", \"chixdiggit\", \"chunk! no, captain chunk!\", \"the click five\",\\\n",
    "              \"the copyrights\", \"count the stars\", \"courage my love\", \"crash romeo\", \"crimpshrine\",\\\n",
    "              \"cub\", \"cute is what we aim for\", \"daggermouth\", \"the dangerous summer\",\\\n",
    "              \"david crowder band\", \"the dead milkmen\", \"descendents\", \"the donnas\", \"the downtown fiction\",\\\n",
    "              \"driving east\", \"dum dums\", \"eat me raw\", \"every avenue\", \"eleventyseven\", \"elliot minor\",\\\n",
    "              \"energy\", \"the ergs!\", \"eternal boy\", \"eve 6\", \"even in blackouts\", \"everyday sunday\",\\\n",
    "              \"faber drive\", \"face to face\", \"falling in reverse\", \"fall out boy\", \"farewell\", \"fenix tx\",\\\n",
    "              \"fight fair\", \"finley\", \"fireworks\", \"flatfoot 56\", \"flop\", \"fluf\", \"fm static\", \"fonzie\",\\\n",
    "              \"forever came calling\", \"forever the sickest kids\", \"four year strong\", \"the friday night boys\",\\\n",
    "              \"generation x\", \"the get up kids\", \"ghoti hook\", \"go betty go\", \"go radio\", \"gob\", \"goldfinger\",\\\n",
    "              \"good charlotte\", \"goodnight nurse\", \"grayscale\", \"green day\", \"greyfield\", \"groovie ghoulies\",\\\n",
    "              \"guttermouth\", \"hagfish\", \"halifax\", \"handguns\", \"hangnail\", \"hawk nelson\", \"head injuries\",\\\n",
    "              \"hedley\", \"hey monday\", \"hey violet\", \"hidden in plain view\", \"the hi-fives\", \"the high court\",\\\n",
    "              \"the higher\", \"the hippos\", \"hit the lights\", \"home grown\", \"hot mulligan\", \"house of heroes\",\\\n",
    "              \"houston calls\", \"the huntingtons\", \"i call fives\", \"i fight dragons\", \"icon for hire\",\\\n",
    "              \"ivoryline\", \"the jam\", \"jawbreaker\", \"jeff rosenstock\", \"jimmy eat world\", \"june\",\\\n",
    "              \"just surrender\", \"karate high school\", \"kid down\", \"kids can't fly\", \"kids in glass houses\",\\\n",
    "              \"kids in the way\", \"killerpilze\", \"kiros\", \"kisschasy\", \"knuckle puck\", \"koopa\", \"lagwagon\",\\\n",
    "              \"latterman\", \"left front tire\", \"the leftovers\", \"the lemonheads\", \"less than jake\", \"lifesavors\",\\\n",
    "              \"lifetime\", \"light years\", \"like pacific\", \"the lillingtons\", \"limbeck\", \"lit\", \"lola ray\",\\\n",
    "              \"love you to death\", \"lucky 7\", \"lucky boys confusion\", \"ludo\", \"lustra\", \"mach pelican\",\\\n",
    "              \"magnapop\", \"the maine\", \"makeout\", \"man overboard\", \"manges\", \"marianas trench\",\\\n",
    "              \"masked intruder\", \"the matches\", \"mayday parade\", \"mcbusted\", \"mcfly\",\\\n",
    "              \"me first and the gimme gimmes\", \"melody fall\", \"mest\", \"the methadones\", \"midtown\",\\\n",
    "              \"millencolin\", \"mixtapes\", \"moose blood\", \"momoiro clover z\", \"motion city soundtrack\",\\\n",
    "              \"modern baseball\", \"the movielife\", \"the mr. t experience\", \"the muffs\", \"mxpx\",\\\n",
    "              \"my chemical romance\", \"neck deep\", \"nerf herder\", \"the new cities\", \"new found glory\",\\\n",
    "              \"new years day\", \"nofx\", \"noise by numbers\", \"not by choice\", \"no use for a name\", \"october fall\",\\\n",
    "              \"the offspring\", \"on my honor\", \"one ok rock\", \"orange\", \"over it\", \"paige\", \"parasites\",\\\n",
    "              \"paramore\", \"pardon us\", \"panic! at the disco\", \"patent pending\", \"permanent me\",\\\n",
    "              \"pee wee gaskins\", \"philmont\", \"pierce the veil\", \"plain white t's\", \"plinko\", \"pointed sticks\",\\\n",
    "              \"poor old lu\", \"the presidents of the united states of america\", \"propagandhi\", \"punchbuggy\",\\\n",
    "              \"punchline\", \"pup\", \"the queers\", \"quietdrive\", \"ramones\", \"rancid\", \"real friends\",\\\n",
    "              \"red city radio\", \"the red jumpsuit apparatus\", \"reggie and the full effect\", \"relient k\",\\\n",
    "              \"riddlin' kids\", \"roam\", \"rookie of the year\", \"rudi\", \"rufio\", \"run kid run\", \"saves the day\",\\\n",
    "              \"say anything\", \"saving aimee\", \"scenes from a movie\", \"scott murphy\", \"screeching weasel\",\\\n",
    "              \"seaway\", \"senses fail\", \"set it off\", \"set your goals\", \"shook ones\", \"short stack\", \"showoff\",\\\n",
    "              \"simple plan\", \"sleeping with sirens\", \"sleep on it\", \"slick shoes\", \"sludgeworth\", \"smash mouth\",\\\n",
    "              \"smoking popes\", \"something corporate\", \"son of dork\", \"the soviettes\", \"spanish love songs\",\\\n",
    "              \"sparks the rescue\", \"spazzys\", \"split habit\", \"sr-71\", \"stand atlantic\", \"the starting line\",\\\n",
    "              \"state champs\", \"stellar kart\", \"stereos\", \"steriogram\", \"stiff dylans\", \"story of the year\",\\\n",
    "              \"the story so far\", \"story untold\", \"student rick\", \"sugarcult\", \"sum 41\", \"supergrass\",\\\n",
    "              \"superman is dead\", \"the summer obsession\", \"the summer set\", \"sweet baby\", \"the swellers\",\\\n",
    "              \"tacocat\", \"taking back sunday\", \"teen idols\", \"teenage bottlerocket\", \"ten second epic\",\\\n",
    "              \"terrible things\", \"there for tomorrow\", \"these kids wear crowns\", \"this century\",\\\n",
    "              \"this time next year\", \"tigers jaw\", \"tilt\", \"title fight\", \"tonight alive\", \"transit\",\\\n",
    "              \"trash boat\", \"the travoltas\", \"treble charger\", \"trucks\", \"twenty twenty\", \"undercover\",\\\n",
    "              \"the undertones\", \"the unlovables\", \"unwritten law\", \"valencia\", \"the vandals\", \"vanilla sky\",\\\n",
    "              \"verona grove\", \"violent delight\", \"waterparks\", \"wavves\", \"wakefield\", \"we are the in crowd\",\\\n",
    "              \"the wedding\", \"weezer\", \"we the kings\", \"wheatus\", \"with confidence\", \"the wonder years\",\\\n",
    "              \"wstr\", \"yellowcard\", \"you, me, and everyone we know\", \"you me at six\", \"zebrahead\",\\\n",
    "              \"zolof the rock and roll destroyer\", \"menzingers, the\", \"anberlin\", 'silverstein', 'rise against',\\\n",
    "              \"the used\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure name of bands with \"the\" in front to match dataframe style\n",
    "rest_bands = []\n",
    "for i in bands_punk:\n",
    "    # len of 'the ' is 4; check if first 4 characters are 'the '\n",
    "    if i[:4] == 'the ':\n",
    "        # take out 'the ' from band name and place at end of string\n",
    "        restructured = i[4:] + \", the\"\n",
    "        # append to new list\n",
    "        rest_bands.append(restructured)\n",
    "    elif i[:2] == 'a ':\n",
    "        # take out 'a ' from band name and place at end of string\n",
    "        restructured = i[2:] + \", a\"\n",
    "        # append to new list\n",
    "        rest_bands.append(restructured)\n",
    "    else:\n",
    "        # append name of bands without 'the ' in front\n",
    "        rest_bands.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create punk_df which has the punk rock/pop bands from our list of bands\n",
    "punk_df = all_lyrics[all_lyrics.ARTIST_NAME.isin(rest_bands)]\n",
    "punk_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv\n",
    "punk_df.to_csv('data/punk_bands.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARTIST_NAME</th>\n",
       "      <th>ARTIST_URL</th>\n",
       "      <th>SONG_NAME</th>\n",
       "      <th>SONG_URL</th>\n",
       "      <th>LYRICS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 seconds of summer</td>\n",
       "      <td>https://www.azlyrics.com/19/5secondsofsummer.html</td>\n",
       "      <td>gotta get out</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/5secondsofsumm...</td>\n",
       "      <td>even when the sky is falling down even when th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5 seconds of summer</td>\n",
       "      <td>https://www.azlyrics.com/19/5secondsofsummer.html</td>\n",
       "      <td>better man</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/5secondsofsumm...</td>\n",
       "      <td>find me at a quarter to three cigarette in my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 seconds of summer</td>\n",
       "      <td>https://www.azlyrics.com/19/5secondsofsummer.html</td>\n",
       "      <td>more</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/5secondsofsumm...</td>\n",
       "      <td>if me and you are living in the same place why...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 seconds of summer</td>\n",
       "      <td>https://www.azlyrics.com/19/5secondsofsummer.html</td>\n",
       "      <td>why won't you love me</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/5secondsofsumm...</td>\n",
       "      <td>switching into airplane mode again we're not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5 seconds of summer</td>\n",
       "      <td>https://www.azlyrics.com/19/5secondsofsummer.html</td>\n",
       "      <td>woke up in japan</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/5secondsofsumm...</td>\n",
       "      <td>i woke up in japan feeling low feeling lonely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>zebrahead</td>\n",
       "      <td>https://www.azlyrics.com/z/zebrahead.html</td>\n",
       "      <td>out of control</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/zebrahead/outo...</td>\n",
       "      <td>i'm a mad man with a mission like a nightmare ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>zebrahead</td>\n",
       "      <td>https://www.azlyrics.com/z/zebrahead.html</td>\n",
       "      <td>photographs</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/zebrahead/phot...</td>\n",
       "      <td>i am alive i am awake not like i'm not aware i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>zebrahead</td>\n",
       "      <td>https://www.azlyrics.com/z/zebrahead.html</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/zebrahead/poli...</td>\n",
       "      <td>firewall school halls cop cars protecting you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>zebrahead</td>\n",
       "      <td>https://www.azlyrics.com/z/zebrahead.html</td>\n",
       "      <td>with legs like that</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/zebrahead/with...</td>\n",
       "      <td>here she comes again like good medicine every ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>zebrahead</td>\n",
       "      <td>https://www.azlyrics.com/z/zebrahead.html</td>\n",
       "      <td>wookie</td>\n",
       "      <td>https://www.azlyrics.com/lyrics/zebrahead/wook...</td>\n",
       "      <td>in a cantina on tatoinea a kereoki wookie sing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ARTIST_NAME                                         ARTIST_URL  \\\n",
       "0     5 seconds of summer  https://www.azlyrics.com/19/5secondsofsummer.html   \n",
       "1     5 seconds of summer  https://www.azlyrics.com/19/5secondsofsummer.html   \n",
       "2     5 seconds of summer  https://www.azlyrics.com/19/5secondsofsummer.html   \n",
       "3     5 seconds of summer  https://www.azlyrics.com/19/5secondsofsummer.html   \n",
       "4     5 seconds of summer  https://www.azlyrics.com/19/5secondsofsummer.html   \n",
       "...                   ...                                                ...   \n",
       "2512            zebrahead          https://www.azlyrics.com/z/zebrahead.html   \n",
       "2513            zebrahead          https://www.azlyrics.com/z/zebrahead.html   \n",
       "2514            zebrahead          https://www.azlyrics.com/z/zebrahead.html   \n",
       "2515            zebrahead          https://www.azlyrics.com/z/zebrahead.html   \n",
       "2516            zebrahead          https://www.azlyrics.com/z/zebrahead.html   \n",
       "\n",
       "                  SONG_NAME  \\\n",
       "0             gotta get out   \n",
       "1                better man   \n",
       "2                      more   \n",
       "3     why won't you love me   \n",
       "4          woke up in japan   \n",
       "...                     ...   \n",
       "2512         out of control   \n",
       "2513            photographs   \n",
       "2514               politics   \n",
       "2515    with legs like that   \n",
       "2516                 wookie   \n",
       "\n",
       "                                               SONG_URL  \\\n",
       "0     https://www.azlyrics.com/lyrics/5secondsofsumm...   \n",
       "1     https://www.azlyrics.com/lyrics/5secondsofsumm...   \n",
       "2     https://www.azlyrics.com/lyrics/5secondsofsumm...   \n",
       "3     https://www.azlyrics.com/lyrics/5secondsofsumm...   \n",
       "4     https://www.azlyrics.com/lyrics/5secondsofsumm...   \n",
       "...                                                 ...   \n",
       "2512  https://www.azlyrics.com/lyrics/zebrahead/outo...   \n",
       "2513  https://www.azlyrics.com/lyrics/zebrahead/phot...   \n",
       "2514  https://www.azlyrics.com/lyrics/zebrahead/poli...   \n",
       "2515  https://www.azlyrics.com/lyrics/zebrahead/with...   \n",
       "2516  https://www.azlyrics.com/lyrics/zebrahead/wook...   \n",
       "\n",
       "                                                 LYRICS  \n",
       "0     even when the sky is falling down even when th...  \n",
       "1     find me at a quarter to three cigarette in my ...  \n",
       "2     if me and you are living in the same place why...  \n",
       "3      switching into airplane mode again we're not ...  \n",
       "4     i woke up in japan feeling low feeling lonely ...  \n",
       "...                                                 ...  \n",
       "2512  i'm a mad man with a mission like a nightmare ...  \n",
       "2513  i am alive i am awake not like i'm not aware i...  \n",
       "2514  firewall school halls cop cars protecting you ...  \n",
       "2515  here she comes again like good medicine every ...  \n",
       "2516  in a cantina on tatoinea a kereoki wookie sing...  \n",
       "\n",
       "[2517 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create seed for reusability\n",
    "random.seed(a = 11)\n",
    "\n",
    "#create list of subset of punk bands\n",
    "subset_punk = []\n",
    "\n",
    "# subset only using 1/3 bands found in the dataframe\n",
    "while len(subset_punk) < round(len(np.unique(punk_df.ARTIST_NAME))/3):\n",
    "    # randomly choose a band from the list\n",
    "    chosen = random.choice(np.unique(punk_df.ARTIST_NAME))\n",
    "    # if band not in the list, append the band to the list\n",
    "    if chosen not in subset_punk:\n",
    "        subset_punk.append(chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_punk_df = all_lyrics[all_lyrics.ARTIST_NAME.isin(subset_punk)]\n",
    "subset_punk_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv\n",
    "subset_punk_df.to_csv('data/subset_punk_bands.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pseudo Code for splitting lyrics (based on commas)\n",
    "```python\n",
    "split_lyrics_lst= []\n",
    "for row in punk df:\n",
    "    split_lyr = row.split(', ')\n",
    "    rows = []\n",
    "    for line in split_lyr:\n",
    "        rows.append(name, song name, line)\n",
    "        \n",
    "    split_lyrics_lst.append(rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Input, Embedding, Dropout, concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For artist train/test, maybe while creating the padded sequences, also create a list with the artist appended to it.\n",
    "So in the for loop where n_gram_seq is created, put an outer for loop for each artist? This might be for every row in the ARTIST_NAME column (YC only has 52 songs in df and token_seq has 52 length, maybe this can be done in one for loop with token_seq (potentially enumerate this & create an empty list for the artists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2414    possessions the last thing on my mind i don't ...\n",
       "2415    the land of the free the home the deprived for...\n",
       "2416    everytime i try to read between the lines i'm ...\n",
       "2417    we'll leave this shit behind our moms our dads...\n",
       "2418     woohoohoohoo for the longest time woohoohoo f...\n",
       "2419    we're jamming in the bedroom mom will be home ...\n",
       "2420    you hung me out to dry you said that we're thr...\n",
       "2421    it's six in the morning you still haven't slep...\n",
       "2422    seeing what the cause has left us there's no s...\n",
       "2423    well if it was just in life then we'd know whe...\n",
       "2424    five teens involved uh one of them appears to ...\n",
       "2425     let's burn a hole so we can climb out of thes...\n",
       "2426    this is the deepest cut i think i have ever fe...\n",
       "2427    are you there putting all the words together p...\n",
       "2428    4 am you call to spit some fire out did you th...\n",
       "2429    do you remember when i said you were my only o...\n",
       "2430    i can not hold this anymore my hands are tired...\n",
       "2431    i can see all the footsteps left behind every ...\n",
       "2432    haven't heard from you in days i'm spreading t...\n",
       "2433    we seem to have the world here in our hands it...\n",
       "2434    look at me and listen close so i can tell you ...\n",
       "2435    the sun comes up and you are all over my mind ...\n",
       "2436    i made these wishes with you went coast to coa...\n",
       "2437    joking every year it's an anniversary you spok...\n",
       "2438    bottoms up tonight i drink to you and i 'cause...\n",
       "2439    this is a story full of restless nights of do ...\n",
       "2440    i think i have everything i need it got dark b...\n",
       "2441    if i could write to the kid i was before i'd t...\n",
       "2442    i took a flight headed north i wasn't waiting ...\n",
       "2443    let me say this first i was on your side befor...\n",
       "2444    i've been here a while staring at this screen ...\n",
       "2445    here i go again another leap of faith i close ...\n",
       "2446    mmmmm mmmmm i found out in the fall i've been ...\n",
       "2447    i've watched the world go by outside a window ...\n",
       "2448    when you try your best but you don't succeed w...\n",
       "2449    i'm here reaching out again lead into the wind...\n",
       "2450    something here it never looks right we've been...\n",
       "2451    turn around before you go the look in your eye...\n",
       "2452    it's hard to think where we began we were born...\n",
       "2453    take me home take me home i am lost in the wor...\n",
       "2454    i see the barricades ahead roll my sleeves up ...\n",
       "2455    if a cold wind starts to rise i am ready now i...\n",
       "2456    storybooks i thought were written for my heart...\n",
       "2457    i am on my mountain i am there with her i can ...\n",
       "2458    tell me secrets 'cause i know you're scared an...\n",
       "2459    you got secrets in your heart i got mysteries ...\n",
       "2460    it was the best that you could be for me i thi...\n",
       "2461    slow steady hands waving their last goodbye th...\n",
       "2462    stacking bricks on broken ground building towe...\n",
       "2463    you feel it you boxed it by the youth you left...\n",
       "2464    so where are you and how's it been how's the w...\n",
       "2465    watch winter melt away look for longer days th...\n",
       "Name: LYRICS, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellowcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yellowcard'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punk_df.ARTIST_NAME.iloc[2465]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"randomly\" choose one band for model\n",
    "yellowcard = punk_df.LYRICS[punk_df.ARTIST_NAME == 'yellowcard']\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(yellowcard)\n",
    "\n",
    "token_seq = tokenizer.texts_to_sequences(yellowcard)\n",
    "\n",
    "n_gram_seq = []\n",
    "artists = []\n",
    "# for every line in tokenized sequences\n",
    "for idx, line in enumerate(token_seq):\n",
    "    # used to append the token_seq starting from 0th element to 1st element\n",
    "    for length in range(2, len(line)):\n",
    "        n_gram_seq.append(line[:length])\n",
    "        # adding 2414 since this is where yellowcard starts\n",
    "        artists.append(punk_df.ARTIST_NAME.iloc[idx+2414])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yellowcard'], dtype='<U10')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create padded sequences\n",
    "n_gram_seq_padded = pad_sequences(n_gram_seq, maxlen = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0, 704,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 704,   3, 108],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 704,   3, 108, 158],\n",
       "       [  0,   0,   0,   0,   0,   0, 704,   3, 108, 158,  14],\n",
       "       [  0,   0,   0,   0,   0, 704,   3, 108, 158,  14,   9],\n",
       "       [  0,   0,   0,   0, 704,   3, 108, 158,  14,   9, 204],\n",
       "       [  0,   0,   0, 704,   3, 108, 158,  14,   9, 204,   1],\n",
       "       [  0,   0, 704,   3, 108, 158,  14,   9, 204,   1,  63],\n",
       "       [  0, 704,   3, 108, 158,  14,   9, 204,   1,  63, 113],\n",
       "       [704,   3, 108, 158,  14,   9, 204,   1,  63, 113,   8]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_seq_padded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels by using One Hot Encoding \n",
    "labels = to_categorical(n_gram_seq_padded[:,-1:])\n",
    "X = n_gram_seq_padded[:,:-1]\n",
    "\n",
    "train_size = round(n_gram_seq_padded.shape[0]*0.8)\n",
    "\n",
    "# create test and train\n",
    "y_train = labels[:train_size:]\n",
    "y_test = labels[train_size:,:]\n",
    "\n",
    "lyrics_train = X[:train_size,:]\n",
    "lyrics_test = X[train_size:,:]\n",
    "\n",
    "artist_train = np.full((lyrics_train.shape[0], lyrics_train.shape[1]), 'yellowcard')\n",
    "artist_test = np.full((lyrics_test.shape[0], lyrics_test.shape[1]), 'yellowcard')\n",
    "\n",
    "# find largest vocab size in padded sequence; this is input size\n",
    "vocab_size = max([w for sentence in n_gram_seq_padded for w in sentence]) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the two inputs\n",
    "inputA = Input(shape = (10, ))\n",
    "# inputB = Input(shape = (128, ))\n",
    "\n",
    "# first branch for first input\n",
    "lyrics = Embedding(input_dim = vocab_size, output_dim = 64, input_length = 11)(inputA)\n",
    "lyrics = Bidirectional(LSTM(128, return_sequences = True))(lyrics)\n",
    "lyrics = Dropout(0.2)(lyrics)\n",
    "lyrics = LSTM(64)(lyrics)\n",
    "lyrics = Dense(round(vocab_size/2), activation = 'relu')(lyrics)\n",
    "lyrics = Dense(vocab_size, activation = 'softmax')(lyrics)\n",
    "lyrics = Model(inputs = inputA, outputs = lyrics)\n",
    "\n",
    "# # second branch for second input\n",
    "# artist = Dense(64, activation = 'relu')(inputB)\n",
    "# artist = Dense(32, activation = 'relu')(artist)\n",
    "# artist = Dense(4, activation = 'relu')(artist)\n",
    "# artist = Model(inputs = inputB, outputs = artist)\n",
    "\n",
    "\n",
    "# combine output of branches\n",
    "# combined = concatenate([lyrics.output, artist.output])\n",
    "\n",
    "\n",
    "z = Dense(2, activation = 'relu')(lyrics.output)\n",
    "z = Dense(1, activation = 'softmax')(z)\n",
    "model = keras.Model(inputs = lyrics.input, outputs = z)\n",
    "\n",
    "#compile model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 10, 64)            74304     \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 10, 256)           197632    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 64)                82176     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 580)               37700     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1161)              674541    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 2324      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 1,068,680\n",
      "Trainable params: 1,068,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/belindanguyen/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (32, 1161) and (32, 1) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-38dbbf3ee28f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlyrics_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlyrics_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    803\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;34m\"\"\"Runs a training execution with one step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m    797\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1257\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1258\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1259\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2728\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2732\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3415\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3417\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3419\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    754\u001b[0m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m       loss = self.compiled_loss(\n\u001b[0;32m--> 756\u001b[0;31m           y, y_pred, sample_weight, regularization_losses=self.losses)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_dtype_and_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0mloss_metric_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    150\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[1;32m    154\u001b[0m           losses, sample_weight, reduction=self._get_reduction())\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    254\u001b[0m           y_pred, y_true)\n\u001b[1;32m    255\u001b[0m     \u001b[0mag_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing)\u001b[0m\n\u001b[1;32m   1535\u001b[0m   y_true = smart_cond.smart_cond(label_smoothing, _smooth_labels,\n\u001b[1;32m   1536\u001b[0m                                  lambda: y_true)\n\u001b[0;32m-> 1537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   4831\u001b[0m   \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4833\u001b[0;31m   \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4835\u001b[0m   \u001b[0;31m# Use logits whenever they are available. `softmax` and `sigmoid`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \"\"\"\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (32, 1161) and (32, 1) are incompatible"
     ]
    }
   ],
   "source": [
    "model.fit([lyrics_train], y_train, epochs = 20, validation_data = (lyrics_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyrics(prompt, length):\n",
    "    '''\n",
    "    prompt: string of lyrics\n",
    "    length: length of lyrics that is wanted (includes prompt)\n",
    "    '''\n",
    "    # edge case; if prompt is as long as the length wanted\n",
    "    if len(prompt.split(' ')) == length:\n",
    "        return prompt\n",
    "    else:\n",
    "        for _ in range(length - len(prompt.split(' '))):\n",
    "            \n",
    "            token_list = tokenizer.texts_to_sequences([prompt])[0]\n",
    "            token_padded = pad_sequences([token_list], maxlen = 11)\n",
    "            \n",
    "            # get predicted probability for each word\n",
    "            predicted_probs = model.predict(token_padded)[0]\n",
    "            \n",
    "            # find max probability of the each word\n",
    "            word_choice = predicted_probs.argmax()\n",
    "            \n",
    "            # find out what the word is\n",
    "            output_word = tokenizer.index_word[word_choice]\n",
    "            \n",
    "            # add word to the prompt\n",
    "            prompt += ' ' + output_word\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_lyrics(\"eyes are feeling heavy but they never seem to close\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_lyrics(\"there's a place off ocean avenue\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate tags with a number\n",
    "tags_index = dict()\n",
    "for idx, val in enumerate(np.unique(punk_df.ARTIST_NAME)):\n",
    "    tags_index[val] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_docs = []\n",
    "\n",
    "for idx, val in tags_index.items():\n",
    "    # create list for all lyrics\n",
    "    all_lyrics = []\n",
    "    # for all the lyrics of the songs the artist made\n",
    "    for i in punk_df.LYRICS[punk_df.ARTIST_NAME == idx].index:\n",
    "        # tokenize and append each song's lyrics to list\n",
    "        all_lyrics.append(nltk.word_tokenize(punk_df.LYRICS[punk_df.ARTIST_NAME == idx][i]))\n",
    "    # tag all tokenized lyrics with artist's name and append to tagged_docs\n",
    "    tagged_docs.append(TaggedDocument(all_lyrics, tags = idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(epochs = 20, hs = 1, alpha = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[x for x in tagged_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.build_vocab(documents = [x for x in tagged_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type([docs for docs in tagged_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"I love machine learning. Its awesome.\",\n",
    "        \"I love coding in python\",\n",
    "        \"I love building chatbots\",\n",
    "        \"they chat amagingly well\"]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=nltk.word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "m = Doc2Vec()\n",
    "  \n",
    "m.build_vocab(tagged_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
